{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load untitled8.py\n",
    "#!/usr/bin/env python2\n",
    "\"\"\"\n",
    "Created on Fri Dec 15 09:51:52 2017\n",
    "\n",
    "@author: lrl\n",
    "\"\"\"\n",
    "\n",
    "from sklearn.datasets import load_svmlight_file\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def read_date(fileName):\n",
    "    data=load_svmlight_file(fileName)\n",
    "    return data[0],data[1]\n",
    "\n",
    "path_train=\"/home/lrl/Downloads/a9a\"\n",
    "path_test=\"/home/lrl/Downloads/a9a.t\"\n",
    "x_train,y_train=load_svmlight_file(path_train)\n",
    "x_train=x_train.toarray()\n",
    "x_test,y_test=load_svmlight_file(path_test)\n",
    "x_test=x_test.toarray()\n",
    "\n",
    "column=np.zeros((x_test.shape[0]))\n",
    "x_test=np.column_stack((x_test,column))\n",
    "\n",
    "column_train=np.ones((x_train.shape[0]))\n",
    "column_test=np.ones((x_test.shape[0]))\n",
    "x_train=np.column_stack((x_train,column_train))\n",
    "m,n=np.shape(x_train)\n",
    "y_train=y_train.reshape((m,1))\n",
    "x_test=np.column_stack((x_test,column_test))\n",
    "m_train,n_train=np.shape(x_test)\n",
    "y_test=y_test.reshape((m_train,1))\n",
    "print y_test.shape\n",
    "\n",
    "\n",
    "ytest=[]\n",
    "for i in y_test:\n",
    "  \n",
    "    if i == [-1.]:\n",
    "        y_test=[0.]\n",
    "    else:\n",
    "        y_test=i\n",
    "    ytest.append(y_test)\n",
    "    y_test=np.array(ytest)\n",
    "print y_test.shape \n",
    "ytrain=[]\n",
    "for i in y_train:\n",
    "    if i == [-1]:\n",
    "        y_train=[0.]\n",
    "    else:\n",
    "        y_train=i\n",
    "    ytrain.append(y_train)\n",
    "    y_train=np.array(ytrain)\n",
    "print y_train.shape\n",
    "def logigradient(x,y,w,eta,c):\n",
    "    sigmoid=1/(1+np.exp(-np.dot(x,w)))\n",
    "    #print x.shape\n",
    "   \n",
    "    #print sigmoid.shape\n",
    "    grad=np.dot(x.T,(sigmoid-y))\n",
    "    #print np.log(sigmoid).shape\n",
    "    cost=-np.average(y*np.log(sigmoid)+(1-y)*np.log(1-sigmoid))\n",
    "    #print np.log(sigmoid).shape\n",
    "  \n",
    "    return cost,grad\n",
    "  \n",
    " \n",
    "def draw_plot(Loss_train, Loss_test, name):\n",
    "    plt.figure(figsize=(20,15))\n",
    "    plt.plot(Loss_train,color=\"r\",label=\"Loss_train\")\n",
    "    plt.plot(Loss_test,color=\"b\",label=\"Loss_validation\")\n",
    "    plt.legend()\n",
    "    plt.xlabel(\"epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(\"Logistic regression optimized by \"+ name)\n",
    "    plt.show()\n",
    "    \n",
    "def NAG(x,y):\n",
    "    epoch=200\n",
    "    gamma=0.7\n",
    "    m,n=np.shape(x)\n",
    "    w=np.random.random((n,1))\n",
    "    v=np.array(np.zeros(w.shape))\n",
    " \n",
    "    costs=[]\n",
    "    for epoch in range(epoch):\n",
    "      \n",
    "        eta=0.03\n",
    "        Loss,grad=logigradient(x,y,w-gamma*v,eta,gamma)\n",
    "        v=gamma*v+eta*grad\n",
    "        w=w-v\n",
    "        costs.append(Loss)\n",
    "    return costs,w\n",
    "Loss_train1,grad=NAG(x_train,y_train)\n",
    "Loss_test1,grad=NAG(x_test,y_test)\n",
    "draw=draw_plot(Loss_train1, Loss_test1, 'NAG')\n",
    "\n",
    "def Adam(x,y):\n",
    "    epoch=200\n",
    "    costs=[]\n",
    "    c=0.2\n",
    "    m,n=np.shape(x)\n",
    "    w=np.random.random((n,1))\n",
    "    v=np.array(np.zeros(w.shape))\n",
    "    beta1=0.8\n",
    "    beta2=0.989\n",
    "    beta1_exp=1.0\n",
    "    beta2_exp=1.0\n",
    "    eta=1\n",
    "    epsilon=1e-8\n",
    "    \n",
    "    for epoch in range(epoch):\n",
    "        cost,grad=logigradient(x,y,w,eta,c)\n",
    "        m=beta1*m+(1.0-beta1)*grad\n",
    "        v=beta2*v+(1.0-beta2)*np.square(grad)\n",
    "        beta1_exp*=beta1\n",
    "        beta2_exp*=beta2\n",
    "        w=w-eta*(m/(1.0-beta1_exp))/(np.sqrt(v/(1.0-beta2_exp))+epsilon)\n",
    "        costs.append(cost)\n",
    "    return costs,w\n",
    "Loss_train1,grad=Adam(x_train,y_train)\n",
    "Loss_test1,grad=Adam(x_test,y_test)\n",
    "draw=draw_plot(Loss_train1, Loss_test1, 'Adam')\n",
    "\n",
    "\n",
    "def AdaDelta(x,y):\n",
    "    epoch=200\n",
    "    c=0.3\n",
    "    gamma=1.0\n",
    "    epsilon=1e-8\n",
    "    m,n=np.shape(x)\n",
    "    w=np.random.random((n,1))\n",
    "    grad_expect=np.array(np.zeros(w.shape))\n",
    "    delta_expect=np.array(np.zeros(w.shape))\n",
    "    \n",
    "    costs=[]\n",
    "    eta=0.0001\n",
    "    for epoch in range(epoch):\n",
    "        cost,grad=logigradient(x,y,w,eta,c)\n",
    "        grad_expect=gamma*grad_expect+(1.0-gamma)*np.square(grad)\n",
    "        delta=-np.multiply(np.sqrt(delta_expect+epsilon)/np.sqrt(grad_expect+epsilon),grad)\n",
    "        w=w+delta\n",
    "        delta_expect=gamma*delta_expect+(1.0-gamma)*np.square(delta)\n",
    "        costs.append(cost)\n",
    "    return costs,w\n",
    "Loss_train1,grad=AdaDelta(x_train,y_train)\n",
    "Loss_test1,grad=AdaDelta(x_test,y_test)\n",
    "draw=draw_plot(Loss_train1, Loss_test1, 'AdaDelta')\n",
    "\n",
    "def RSMProp(x,y):\n",
    "    epoch=200\n",
    "    m,n=np.shape(x)\n",
    "    w=np.random.random((n,1))\n",
    "    gamma=0.99\n",
    "    costs=[]\n",
    "    eta=0.01\n",
    "    epsilon=1e-8\n",
    "    grad_expect=np.array(np.zeros(w.shape))\n",
    "   \n",
    "    for epoch in range(epoch):\n",
    "        cost,grad=logigradient(x,y,w,eta,c)\n",
    "        grad_expect=gamma*grad_expect+(1.0-gamma)*np.square(grad)\n",
    "        w=w-eta*grad/np.sqrt(grad_expect+epsilon)\n",
    "     \n",
    "        costs.append(cost)\n",
    "    return costs,w\n",
    "Loss_train1,grad=RSMProp(x_train,y_train)\n",
    "Loss_test1,grad=RSMProp(x_test,y_test)\n",
    "draw=draw_plot(Loss_train1, Loss_test1, 'RSMProp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
